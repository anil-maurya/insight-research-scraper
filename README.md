# ğŸ“˜ User Insight Data Collection Pipeline

A lightweight, founder-friendly system for collecting, cleaning, and analyzing user comments from public platforms (Reddit, YouTube, App Store, Instagram, etc.).

Designed for:

- **Insight generation**
- **User psychology analysis**
- **Competitor gap identification**
- **Product development clarity**

Not designed for:

- Heavy data engineering
- Complex ML pipelines
- Data science workflows

This system is intentionally simple.

---

## ğŸ—ï¸ **Repository Structure**

```
data-collection/
â”‚
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ reddit_scraper.py
â”‚   â”œâ”€â”€ youtube_scraper.py
â”‚   â”œâ”€â”€ instagram_scraper.py   (optional)
â”‚   â”œâ”€â”€ playstore_scraper.py   (optional)
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ clean_text.py
â”‚   â”œâ”€â”€ process_batch.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ schema.json
â”‚
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ run_reddit.sh
â”‚   â”œâ”€â”€ run_youtube.sh
â”‚   â””â”€â”€ run_all.sh
â”‚
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ mongo_connection.py
â”‚   â”œâ”€â”€ insert_raw.py
â”‚   â”œâ”€â”€ insert_processed.py
â”‚   â””â”€â”€ test_connection.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ samples/
â”‚
â”œâ”€â”€ insights/
â”‚   â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ analysis_notes.md
â”‚   â””â”€â”€ final_insights.md
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ credentials.example.json
â”‚   â”œâ”€â”€ mongo.example.json
â”‚   â””â”€â”€ settings.yaml
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ **Purpose of This Pipeline**

This repo gives you:

- A **repeatable way** to collect real user data
- Clean + structured JSON for analysis
- A simple way to store data in **MongoDB**
- A blueprint for running **AI-powered insight extraction**

This is the foundation for:

- Understanding user psychology
- Mapping unmet needs
- Identifying competitor blind spots
- Defining differentiation
- Designing your product in the right direction

---

## ğŸ”§ **Installation**

### 1. Clone the repo

```bash
git clone https://github.com/yourusername/data-collection.git
cd data-collection
```

### 2. Create a virtual environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install requirements

```bash
pip install -r requirements.txt
```

### 4. Create your environment config

Copy the template:

```bash
cp .env.example .env
```

Fill in:

- Reddit API keys
- YouTube API key
- MongoDB connection string

---

## ğŸ§± **How It Works (Simple Overview)**

### **1. Run a scraper**

Example:

```bash
bash pipelines/run_reddit.sh
```

This will:

- Fetch comments
- Save raw JSON in `data/raw/reddit/`
- Insert raw JSON into MongoDB

---

### **2. Run ETL processing**

```bash
python etl/process_batch.py
```

This will:

- Clean text
- Normalize structure
- Save processed JSON in `data/processed/`
- Insert processed data into MongoDB

---

### **3. Analyze inside ChatGPT/Claude**

Copy the contents of:

```
insights/prompts/*
```

Use prompts like:

- â€œCluster these comments into 10 themes.â€
- â€œExtract top emotional triggers.â€
- â€œIdentify unmet needs.â€
- â€œMap competitor failures from these comments.â€

Document findings in:

```
insights/analysis_notes.md
```

---

## ğŸ—„ï¸ **MongoDB Collections**

Two collections are used:

### **1. raw_comments**

- Stores exact raw scraped comments
- No cleaning
- Useful for reruns and audits

### **2. processed_comments**

- Cleaned text
- Normalized structure
- Useful for AI analysis and search

---

## ğŸ“¥ **Adding New Platforms**

Add a new scraper file in `/scrapers/`.
Output raw JSON to `/data/raw/<platform>/`.

Update pipeline scripts in `/pipelines/` if needed.

---

## ğŸš« **Avoid Analysis Paralysis**

Follow these rules:

### **Rule 1: Limit scraping**

- 200â€“500 comments per platform
- No infinite pagination
- Quality > quantity

### **Rule 2: Limit cleaning**

Do basic cleaning only:

- lowercase
- remove emojis
- strip whitespace

### **Rule 3: Limit analysis passes**

- 1st pass = themes
- 2nd pass = persona notes
- STOP after pass 2

### **Rule 4: Set a deadline**

Complete the entire data collection stage in **7â€“10 days**.

---

## ğŸ§ª **Testing Your Setup**

Test MongoDB connection:

```bash
python storage/test_connection.py
```

Test Reddit scraper (first run):

```bash
python scrapers/reddit_scraper.py
```

Check data folder:

```
data/raw/reddit/*.json
```

---

## ğŸ§  **Insight Workflow Summary**

```
Scrape â†’ Clean â†’ Store â†’ AI Analyze â†’ Insight Report
```

Outputs:

- User pains
- Desires
- Emotional triggers
- Competitor gaps
- Positioning opportunities
- Product direction clarity

This is your **Market Insight Engine**.

---

## ğŸª„ **Future Enhancements (Optional)**

These are optional â€” only if needed later:

- Add Elasticsearch for faster search
- Add vector DB for semantic search
- Automate pipelines via cron
- Dashboard with Metabase

But for now:
**Keep it simple. Keep moving. No overbuilding.**
